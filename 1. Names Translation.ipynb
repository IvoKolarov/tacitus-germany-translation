{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MHz18R7sVeCg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from io import StringIO\n",
        "from dotenv import load_dotenv\n",
        "from typing import Any, Dict, List, TypedDict\n",
        "import pandas as pd\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from prompt_templates import name_extraction_template, name_translation_template\n",
        "from utilities import batch_list, extract_csv, extract_json\n",
        "\n",
        "load_dotenv() # Loading the API key from a .env file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-5\" # Cost and latency are not that much of a factor at this text size, hence the choice of a model. \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Processing\n",
        "\n",
        "- The book is split into c.a. 40 chunks using recursive splitting.\n",
        "- The paragraphs of the text are relatively short and the chunk size chosen preserves them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4xQNAncXXX-l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text chunks: 39\n"
          ]
        }
      ],
      "source": [
        "with open(\"text_clean.txt\", \"r\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\"],\n",
        "    chunk_size=2_500,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(text)\n",
        "print(f\"Number of text chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Extraction and Initial Translation\n",
        "\n",
        " - Each chunk passes through chunk_app.\n",
        " - An LLM extracts the names of people, peoples and places and makes an initial translation.\n",
        " - The output for each paragraph is converted into tabular format for an easier review by a human later.\n",
        " - The chain is not executed directly, it is component of global_app, which executes chunk_app in parallel.\n",
        " - global_app processes text chunks in batches via chunk_app.\n",
        " - The translations are aggregated into a single DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bqEP9Nh4jKvM"
      },
      "outputs": [],
      "source": [
        "class ChunkState(TypedDict):\n",
        "    text: str\n",
        "    extracted_json: dict\n",
        "    translation_csv: str\n",
        "    parsed_table: pd.DataFrame\n",
        "\n",
        "async def extract_names(state: ChunkState) -> Dict[str, dict]:\n",
        "    prompt = name_extraction_template.format_messages(text=state[\"text\"])\n",
        "    response = await llm.ainvoke(prompt)\n",
        "    extracted = extract_json(response.content)\n",
        "    return {\"extracted_json\": extracted}\n",
        "\n",
        "async def translate_names(state: ChunkState) -> Dict[str, str]:\n",
        "    names_json = json.dumps(state[\"extracted_json\"], ensure_ascii=False, indent=2)\n",
        "    prompt = name_translation_template.format_messages(names_in_english=names_json)\n",
        "    response = await llm.ainvoke(prompt)\n",
        "    return {\"translation_csv\": response.content}\n",
        "\n",
        "async def parse_translation(state: ChunkState) -> Dict[str, pd.DataFrame]:\n",
        "    if \"csv\" in state[\"translation_csv\"]: # The API response will occasionally start with \"```csv\", that's handled here.\n",
        "        csv_string = extract_csv(state[\"translation_csv\"])\n",
        "    else:\n",
        "        csv_string = state[\"translation_csv\"]\n",
        "\n",
        "    df = pd.read_csv(StringIO(csv_string))\n",
        "    return {\"parsed_table\": df}\n",
        "\n",
        "chunk_graph = StateGraph(ChunkState)\n",
        "\n",
        "chunk_graph.add_node(\"extract_names\", extract_names)\n",
        "chunk_graph.add_node(\"translate_names\", translate_names)\n",
        "chunk_graph.add_node(\"parse_translation\", parse_translation)\n",
        "\n",
        "chunk_graph.add_edge(START, \"extract_names\")\n",
        "chunk_graph.add_edge(\"extract_names\", \"translate_names\")\n",
        "chunk_graph.add_edge(\"translate_names\", \"parse_translation\")\n",
        "chunk_graph.add_edge(\"parse_translation\", END)\n",
        "\n",
        "chunk_app = chunk_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalState(TypedDict):\n",
        "    texts: List[str]\n",
        "    chunk_results: List[ChunkState]\n",
        "    aggregated: pd.DataFrame\n",
        "\n",
        "async def aggregate_results(state: GlobalState) -> Dict[str, pd.DataFrame]:\n",
        "    dfs = [chunk[\"parsed_table\"] for chunk in state[\"chunk_results\"]]\n",
        "    merged_df = pd.concat(dfs, ignore_index=True)\n",
        "    return {\"aggregated\": merged_df}\n",
        "\n",
        "async def run_chunk_pipeline(state: GlobalState, batch_size: int = 10) -> Dict[str, list]:\n",
        "    all_results = []\n",
        "    for batch in batch_list(state[\"texts\"], batch_size):\n",
        "        tasks = [chunk_app.ainvoke({\"text\": t}) for t in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        all_results.extend(results)\n",
        "    return {\"chunk_results\": all_results}\n",
        "\n",
        "async def process_chunks(state: GlobalState) -> Dict[str, list]:\n",
        "    return await run_chunk_pipeline(state, batch_size=10)\n",
        "\n",
        "# Build graph\n",
        "global_graph = StateGraph(GlobalState)\n",
        "global_graph.add_node(\"process_chunks\", process_chunks)\n",
        "global_graph.add_node(\"aggregate\", aggregate_results)\n",
        "\n",
        "global_graph.add_edge(START, \"process_chunks\")\n",
        "global_graph.add_edge(\"process_chunks\", \"aggregate\")\n",
        "global_graph.add_edge(\"aggregate\", END)\n",
        "\n",
        "global_app = global_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph execution.\n",
        "names_translation = await global_app.ainvoke({\"texts\": chunks})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'names_translation' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Final DataFrame of all translations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mnames_translation\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName in English\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#removing duplicated names\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames extracted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'names_translation' is not defined"
          ]
        }
      ],
      "source": [
        "# Final DataFrame of all translations\n",
        "df = names_translation[\"aggregated\"]\n",
        "df.drop_duplicates(subset=\"Name in English\", inplace=True) #removing duplicated names\n",
        "print(f\"Names extracted: {df.shape[0]}\")\n",
        "df.sort_values(\"Name in English\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the results in an excel file for audit.\n",
        "df.to_excel(\"names_translation_unaudited.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
