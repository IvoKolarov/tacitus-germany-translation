{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MHz18R7sVeCg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from io import StringIO\n",
        "from dotenv import load_dotenv\n",
        "from typing import Any, Dict, List, TypedDict\n",
        "import pandas as pd\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from prompt_templates import name_extraction_template, name_translation_template\n",
        "from utilities import batch_list, extract_csv, extract_json\n",
        "\n",
        "load_dotenv() # Loading the API key from a .env file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-5\" # Cost and latency are not that much of a factor at this text size, hence the choice of a model. \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Processing\n",
        "\n",
        "- The book is split into c.a. 40 chunks using recursive splitting.\n",
        "- The paragraphs of the text are relatively short and the chunk size chosen preserves them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4xQNAncXXX-l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text chunks: 39\n"
          ]
        }
      ],
      "source": [
        "with open(\"text_clean.txt\", \"r\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\"],\n",
        "    chunk_size=2_500,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(text)\n",
        "print(f\"Number of text chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Extraction and Initial Translation\n",
        "\n",
        " - Each chunk passes through chunk_app.\n",
        " - An LLM extracts the names of people, peoples and places and makes an initial translation.\n",
        " - The output for each paragraph is converted into tabular format for an easier review by a human later.\n",
        " - The chain is not executed directly, it is component of global_app, which executes chunk_app in parallel.\n",
        " - global_app processes text chunks in batches via chunk_app.\n",
        " - The translations are aggregated into a single DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bqEP9Nh4jKvM"
      },
      "outputs": [],
      "source": [
        "class ChunkState(TypedDict):\n",
        "    text: str\n",
        "    extracted_json: dict\n",
        "    translation_csv: str\n",
        "    parsed_table: pd.DataFrame\n",
        "\n",
        "async def extract_names(state: ChunkState) -> Dict[str, dict]:\n",
        "    prompt = name_extraction_template.format_messages(text=state[\"text\"])\n",
        "    response = await llm.ainvoke(prompt)\n",
        "    extracted = extract_json(response.content)\n",
        "    return {\"extracted_json\": extracted}\n",
        "\n",
        "async def translate_names(state: ChunkState) -> Dict[str, str]:\n",
        "    names_json = json.dumps(state[\"extracted_json\"], ensure_ascii=False, indent=2)\n",
        "    prompt = name_translation_template.format_messages(names_in_english=names_json)\n",
        "    response = await llm.ainvoke(prompt)\n",
        "    return {\"translation_csv\": response.content}\n",
        "\n",
        "async def parse_translation(state: ChunkState) -> Dict[str, pd.DataFrame]:\n",
        "    if \"csv\" in state[\"translation_csv\"]: # The API response will occasionally start with \"```csv\", that's handled here.\n",
        "        csv_string = extract_csv(state[\"translation_csv\"])\n",
        "    else:\n",
        "        csv_string = state[\"translation_csv\"]\n",
        "\n",
        "    df = pd.read_csv(StringIO(csv_string))\n",
        "    return {\"parsed_table\": df}\n",
        "\n",
        "chunk_graph = StateGraph(ChunkState)\n",
        "\n",
        "chunk_graph.add_node(\"extract_names\", extract_names)\n",
        "chunk_graph.add_node(\"translate_names\", translate_names)\n",
        "chunk_graph.add_node(\"parse_translation\", parse_translation)\n",
        "\n",
        "chunk_graph.add_edge(START, \"extract_names\")\n",
        "chunk_graph.add_edge(\"extract_names\", \"translate_names\")\n",
        "chunk_graph.add_edge(\"translate_names\", \"parse_translation\")\n",
        "chunk_graph.add_edge(\"parse_translation\", END)\n",
        "\n",
        "chunk_app = chunk_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalState(TypedDict):\n",
        "    texts: List[str]\n",
        "    chunk_results: List[ChunkState]\n",
        "    aggregated: pd.DataFrame\n",
        "\n",
        "async def aggregate_results(state: GlobalState) -> Dict[str, pd.DataFrame]:\n",
        "    dfs = [chunk[\"parsed_table\"] for chunk in state[\"chunk_results\"]]\n",
        "    merged_df = pd.concat(dfs, ignore_index=True)\n",
        "    return {\"aggregated\": merged_df}\n",
        "\n",
        "async def run_chunk_pipeline(state: GlobalState, batch_size: int = 10) -> Dict[str, list]:\n",
        "    all_results = []\n",
        "    for batch in batch_list(state[\"texts\"], batch_size):\n",
        "        tasks = [chunk_app.ainvoke({\"text\": t}) for t in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        all_results.extend(results)\n",
        "    return {\"chunk_results\": all_results}\n",
        "\n",
        "async def process_chunks(state: GlobalState) -> Dict[str, list]:\n",
        "    return await run_chunk_pipeline(state, batch_size=10)\n",
        "\n",
        "# Build graph\n",
        "global_graph = StateGraph(GlobalState)\n",
        "global_graph.add_node(\"process_chunks\", process_chunks)\n",
        "global_graph.add_node(\"aggregate\", aggregate_results)\n",
        "\n",
        "global_graph.add_edge(START, \"process_chunks\")\n",
        "global_graph.add_edge(\"process_chunks\", \"aggregate\")\n",
        "global_graph.add_edge(\"aggregate\", END)\n",
        "\n",
        "global_app = global_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph execution.\n",
        "names_translation = await global_app.ainvoke({\"texts\": chunks})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Names extracted: 174\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name in English</th>\n",
              "      <th>Name in Bulgarian</th>\n",
              "      <th>Category</th>\n",
              "      <th>Translation type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>AEstyan nations</td>\n",
              "      <td>Естийски народи</td>\n",
              "      <td>names_of_peoples</td>\n",
              "      <td>translation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Abnoba</td>\n",
              "      <td>Абноба</td>\n",
              "      <td>geographical_references</td>\n",
              "      <td>transliteration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Africa</td>\n",
              "      <td>Африка</td>\n",
              "      <td>geographical_references</td>\n",
              "      <td>translation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Agricola</td>\n",
              "      <td>Агрикола</td>\n",
              "      <td>names_of_people</td>\n",
              "      <td>transliteration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>Alcis</td>\n",
              "      <td>Алкис</td>\n",
              "      <td>names_of_people</td>\n",
              "      <td>transliteration</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Name in English Name in Bulgarian                 Category  \\\n",
              "244  AEstyan nations   Естийски народи         names_of_peoples   \n",
              "28            Abnoba            Абноба  geographical_references   \n",
              "44            Africa            Африка  geographical_references   \n",
              "4           Agricola          Агрикола          names_of_people   \n",
              "213            Alcis             Алкис          names_of_people   \n",
              "\n",
              "    Translation type  \n",
              "244      translation  \n",
              "28   transliteration  \n",
              "44       translation  \n",
              "4    transliteration  \n",
              "213  transliteration  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Final DataFrame of all translations\n",
        "df = names_translation[\"aggregated\"]\n",
        "df.drop_duplicates(subset=\"Name in English\", inplace=True) #removing duplicated names\n",
        "print(f\"Names extracted: {df.shape[0]}\")\n",
        "df.sort_values(\"Name in English\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the results in an excel file for audit.\n",
        "df.to_excel(\"names_translation_unaudited.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
