{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MHz18R7sVeCg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import Any, Dict, List, TypedDict\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from prompt_templates import text_translation_template\n",
        "from utilities import batch_list\n",
        "\n",
        "load_dotenv() # Loading the API key from a .env file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-5\" # Cost and latency are not that much of a factor at this text size, hence the choice of a model. \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Processing\n",
        "\n",
        "- The book is split into c.a. 40 chunks using recursive splitting.\n",
        "- The paragraphs of the text are relatively short and the chunk size chosen preserves them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4xQNAncXXX-l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n"
          ]
        }
      ],
      "source": [
        "with open(\"text_clean.txt\", \"r\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    #the below setup splits the text consistently in 1-2 paragraphs, always after sentence completion\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\"],\n",
        "    chunk_size=2_500,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(text)\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constructing Names Dictionary\n",
        "\n",
        "- After auditing the names translation, a dictionary is put together.\n",
        "- Each entry is structured as \\<name in English\\> → \\<name in Bulgarian\\>.\n",
        "- The goal is to inject this information into the text_translation_template prompt to aid the translation.\n",
        "- The format aims to be human readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary examples:\n",
            "\n",
            "_Alcis_ → Алки\n",
            "_Aurinia_ → Ауриния\n",
            "_Boiemum_ → Боемум\n",
            "_Castum_ → Кастум\n",
            "_Germans_ → Германи\n",
            "_Isis_ → Изида\n",
            "_Mannus_ → Манус\n",
            "_Tuisto_ → Туисто\n",
            "_Veleda_ → Веледа\n",
            "Abnoba → Абноба\n",
            "AEstyan nations → естийски народи\n",
            "Africa → Африка\n",
            "Agricola → Агрикола\n",
            "Agrippinensians → агрипи\n",
            "Alcis → Алкис\n",
            "Angles → англи\n",
            "Angrivarians → ангриварии\n",
            "Araviscans → аравискани\n",
            "Arians → ари\n",
            "Arsacides → Аршакиди\n",
            "Asciburgium → Аскибургий\n",
            "Asia → Азия\n",
            "Augustus → Август\n",
            "Aurinia → Ауриния\n",
            "Aviones → авиони\n",
            "Basstarnians → бастарни\n",
            "Ba\n"
          ]
        }
      ],
      "source": [
        "names_df = pd.read_excel(\"names_translation_audited.xlsx\")\n",
        "names_dict = names_df.loc[:,[\"Name in English\", \"Final Translation\"]].to_dict(orient=\"index\")\n",
        "names_dict = {v[\"Name in English\"]:v[\"Final Translation\"] for _, v in names_dict.items()}\n",
        "\n",
        "names_dictionary = \"\"\n",
        "\n",
        "for k, v in names_dict.items():\n",
        "    names_dictionary += f\"{k} → {v}\\n\"\n",
        "\n",
        "print(f\"Dictionary examples:\\n\\n{names_dictionary[:500]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk Translation\n",
        "\n",
        " - Each chunk passes through chunk_translation_app to be translated into Bulgarian.\n",
        " - global_translation_app is used for parallel execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChunkTranslation(TypedDict):\n",
        "    original_text: str\n",
        "    translated_text: str\n",
        "\n",
        "async def translate_chunk(state: ChunkTranslation) -> Dict[str, str]:\n",
        "    prompt = text_translation_template.format_messages(names_dictionary = names_dictionary,\n",
        "                                                        text_chunk = state[\"original_text\"]\n",
        "                                                        )\n",
        "    \n",
        "    translation = await llm.ainvoke(prompt)\n",
        "\n",
        "    return {\"translated_text\":translation.content}\n",
        "\n",
        "chunk_translation_graph = StateGraph(ChunkTranslation)\n",
        "chunk_translation_graph.add_node(\"translate_chunk\", translate_chunk)\n",
        "\n",
        "chunk_translation_graph.add_edge(START, \"translate_chunk\")\n",
        "chunk_translation_graph.add_edge(\"translate_chunk\", END)\n",
        "\n",
        "chunk_translation_app = chunk_translation_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalTranslation(TypedDict):\n",
        "    texts: List[str]\n",
        "    chunk_results: List[ChunkTranslation]\n",
        "\n",
        "async def run_chunk_pipeline(state: GlobalTranslation, batch_size: int = 10) -> Dict[str, list]:\n",
        "    all_results = []\n",
        "    for batch in batch_list(state[\"texts\"], batch_size):\n",
        "        tasks = [chunk_translation_app.ainvoke({\"dictionary\":names_dictionary, \"original_text\": t}) for t in batch]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        all_results.extend(results)\n",
        "    return {\"chunk_results\": all_results}\n",
        "\n",
        "async def process_chunks(state: GlobalTranslation) -> Dict[str, list]:\n",
        "    return await run_chunk_pipeline(state, batch_size=10)\n",
        "\n",
        "# Build graph\n",
        "global_translation_graph = StateGraph(GlobalTranslation)\n",
        "global_translation_graph.add_node(\"process_chunks\", process_chunks)\n",
        "\n",
        "global_translation_graph.add_edge(START, \"process_chunks\")\n",
        "global_translation_graph.add_edge(\"process_chunks\", END)\n",
        "\n",
        "global_translation_app = global_translation_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "translation = await global_translation_app.ainvoke({\"texts\":chunks})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Translated paragraphs are saved alongside the original for easier editing.\n",
        "\n",
        "paragraphs = [\n",
        "\n",
        "f\"\"\"Paragraph #{i}\n",
        "\n",
        "Original:\n",
        "\n",
        "{d[\"original_text\"]}\n",
        "\n",
        "Translation:\n",
        "\n",
        "{d[\"translated_text\"]}\n",
        "\n",
        "==================================================\n",
        "\"\"\"\n",
        "\n",
        "for i, d in enumerate(translation[\"chunk_results\"])\n",
        "                      ]\n",
        "\n",
        "text = \"\\n\".join(paragraphs)\n",
        "\n",
        "with open(\"raw_translation.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
